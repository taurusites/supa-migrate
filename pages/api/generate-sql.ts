// pages/api/generate-sql.ts

import type { NextApiRequest, NextApiResponse } from "next";
import { createClient } from "@supabase/supabase-js";
import { TableSelection, FunctionSelection, TypeSelection, TriggerSelection } from "../../types";

export default async function handler(
  req: NextApiRequest,
  res: NextApiResponse<string | { error: string }>
) {
  try {
    // 1) Supabase creds from headers
    const url = req.headers["x-sb-url"]  as string;
    const key = req.headers["x-sb-key"]  as string;
    if (!url || !key) throw new Error("Missing Supabase URL/key");

    // 2) RPC client for all RPC calls (public schema)
    const rpc = createClient(url, key, {
      global: { headers: { Accept: "application/json" } }
    });

    // 3) Parse selections
    const { selections, functionSelections = [], typeSelections = [], triggerSelections = [] } = req.body as { 
      selections: TableSelection[];
      functionSelections?: FunctionSelection[];
      typeSelections?: TypeSelection[];
      triggerSelections?: TriggerSelection[];
    };
    const picked = selections.filter((s) => s.selected);
    const pickedFunctions = functionSelections.filter((s) => s.selected);
    const pickedTypes = typeSelections.filter((s) => s.selected);
    const pickedTriggers = triggerSelections.filter((s) => s.selected);
    
    if (!picked.length && !pickedFunctions.length && !pickedTypes.length && !pickedTriggers.length) {
      throw new Error("No tables, functions, types, or triggers selected");
    }

    // Build sets for filtering
    const tableSet  = new Set(picked.map((s) => `${s.schema}.${s.table}`));
    const schemaSet = new Set(picked.map((s) => s.schema));

    // Helper to literalize JS values
    const lit = (v: any): string => {
      if (v == null) return "NULL";
      if (typeof v === "string") return `'${v.replace(/'/g,"''")}'`;
      if (typeof v === "object") {
        const js = JSON.stringify(v).replace(/'/g,"''");
        return `'${js}'::jsonb`;
      }
      return v.toString();
    };

    let sql = "-- Generated by Supabase Migration Tool\n\n";

    // ─── STEP 1: ENUM TYPES ─────────────────────────────────────────
    {
      const r = await rpc.rpc("pg_list_enum_types");
      if (r.error) throw r.error;
      const enums = r.data as Array<{
        type_schema: string;
        type_name:   string;
        labels:      string[];
      }> | null;

      const use = enums?.filter((e) => schemaSet.has(e.type_schema)) || [];
      if (use.length) {
        sql += "-- ENUM TYPES\n";
        for (const e of use) {
          const vals = e.labels.map((l) => `'${l.replace(/'/g,"''")}'`);
          sql += `CREATE TYPE ${e.type_schema}.${e.type_name} AS ENUM (${vals.join(
            ", "
          )});\n`;
        }
        sql += "\n";
      }
    }

    // ─── STEP 1.5: USER-DEFINED TYPES ──────────────────────────────────
    if (pickedTypes.length) {
      sql += "-- USER-DEFINED TYPES\n";
      for (const { schema, type } of pickedTypes) {
        try {
          const r = await rpc.rpc("pg_get_type_def", {
            schemaname: schema,
            typename: type
          });
          if (r.error) throw r.error;
          const typeDef = r.data as Array<{ definition: string }> | null;
          if (typeDef && typeDef.length > 0) {
            sql += `${typeDef[0].definition};\n\n`;
          } else {
            sql += `-- Could not retrieve definition for type ${schema}.${type}\n`;
          }
        } catch (err) {
          sql += `-- Error retrieving definition for type ${schema}.${type}: ${err}\n`;
        }
      }
      sql += "\n";
    }

    // ─── STEP 1.6: USER-DEFINED FUNCTIONS ──────────────────────────────
    if (pickedFunctions.length) {
      sql += "-- USER-DEFINED FUNCTIONS\n";
      for (const { schema, function: funcName } of pickedFunctions) {
        try {
          const r = await rpc.rpc("pg_get_function_def", {
            schemaname: schema,
            functionname: funcName
          });
          if (r.error) throw r.error;
          const funcDef = r.data as Array<{ definition: string }> | null;
          if (funcDef && funcDef.length > 0) {
            sql += `${funcDef[0].definition};\n\n`;
          } else {
            sql += `-- Could not retrieve definition for ${schema}.${funcName}\n`;
          }
        } catch (err) {
          sql += `-- Error retrieving definition for ${schema}.${funcName}: ${err}\n`;
        }
      }
      sql += "\n";
    }

    // ─── STEP 1.7: TRIGGERS ─────────────────────────────────────────────
    if (pickedTriggers.length) {
      sql += "-- TRIGGERS\n";
      for (const { schema, trigger, table } of pickedTriggers) {
        try {
          const r = await rpc.rpc("pg_get_trigger_def", {
            schemaname: schema,
            triggername: trigger
          });
          if (r.error) throw r.error;
          const triggerDef = r.data as Array<{ definition: string }> | null;
          if (triggerDef && triggerDef.length > 0) {
            sql += `${triggerDef[0].definition};\n\n`;
          } else {
            sql += `-- Could not retrieve definition for trigger ${schema}.${trigger} on ${table}\n`;
          }
        } catch (err) {
          sql += `-- Error retrieving definition for trigger ${schema}.${trigger}: ${err}\n`;
        }
      }
      sql += "\n";
    }

    // ─── STEP 2: DDL + DATA FOR EACH TABLE ───────────────────────────
    for (const { schema, table } of picked) {
      // 2a) DDL
      {
        const r = await rpc.rpc("pg_get_tabledef", {
          schemaname: schema,
          tablename:  table
        });
        if (r.error) throw r.error;
        const rows = r.data as Array<{ ddl: string }> | null;
        const ddl  = rows?.[0]?.ddl || "";
        sql += `-- DDL for ${schema}.${table}\n${ddl};\n\n`;
      }

      // 2b) Data inserts
      {
        const db = createClient(url, key, {
          db:     { schema },
          global: { headers: { Accept: "application/json" } }
        });
        let offset = 0, pageSize = 500;
        while (true) {
          const { data, error } = await db
            .from(table)
            .select("*")
            .range(offset, offset + pageSize - 1);
          if (error) throw error;
          const rows = data as any[] | null;
          if (!rows || rows.length === 0) break;

          const cols = Object.keys(rows[0]);
          const vals = rows.map((row) =>
            "(" +
            cols.map((c) => lit(row[c])).join(", ") +
            ")"
          );

          sql += `-- Data for ${schema}.${table} OFFSET ${offset}\n`;
          sql += `INSERT INTO ${schema}.${table} (${cols.join(
            ", "
          )}) VALUES\n${vals.join(",\n")};\n\n`;

          if (rows.length < pageSize) break;
          offset += pageSize;
        }
      }
    }

    // ─── STEP 3a: CONSTRAINTS ───────────────────────────────────────
    for (const { schema, table } of picked) {
      const r = await rpc.rpc("pg_list_constraints", {
        schemaname: schema,
        tablename:  table
      });
      if (r.error) throw r.error;
      const cons = r.data as Array<{
        constraint_name: string;
        definition:      string;
      }> | null;
      if (cons && cons.length) {
        sql += `-- Constraints for ${schema}.${table}\n`;
        for (const c of cons) {
          sql += `ALTER TABLE ${schema}.${table}\n  ADD CONSTRAINT ${c.constraint_name} ${c.definition};\n`;
        }
        sql += "\n";
      }
    }

    // ─── STEP 3b: INDEXES (skip any that back a constraint) ─────────
    for (const { schema, table } of picked) {
      // 1) fetch constraint names
      const rc = await rpc.rpc("pg_list_constraints", {
        schemaname: schema,
        tablename:  table
      });
      if (rc.error) throw rc.error;
      const constraintNames = (rc.data as Array<{ constraint_name: string }> | [])
        .map((r) => r.constraint_name);

      // 2) fetch all indexes
      const ri = await rpc.rpc("pg_list_indexes", {
        schemaname: schema,
        tablename:  table
      });
      if (ri.error) throw ri.error;
      const idxs = ri.data as Array<{ indexdef: string }> | [];

      // 3) filter out any whose name matches a constraint
      const use = idxs.filter((ix) => {
        const m = ix.indexdef.match(/INDEX\s+("?)([^\s"]+)\1/i);
        const name = m ? m[2] : null;
        return name != null && !constraintNames.includes(name);
      });

      if (use.length) {
        sql += `-- Indexes for ${schema}.${table}\n`;
        for (const ix of use) {
          sql += ix.indexdef.trim() + ";\n";
        }
        sql += "\n";
      }
    }

    // ─── STEP 3c: FOREIGN KEY CONSTRAINTS ───────────────────────────
    {
      const r = await rpc.rpc("pg_list_foreign_keys");
      if (r.error) throw r.error;
      const fks = r.data as Array<{
        fk_schema:            string;
        fk_name:              string;
        table_schema:         string;
        table_name:           string;
        column_names:         string[];
        foreign_table_schema: string;
        foreign_table_name:   string;
        foreign_column_names: string[];
      }> | null;

      const use = fks?.filter((fk) => {
        const left  = `${fk.table_schema}.${fk.table_name}`;
        const right = `${fk.foreign_table_schema}.${fk.foreign_table_name}`;
        return tableSet.has(left) && tableSet.has(right);
      }) || [];

      if (use.length) {
        sql += "-- FOREIGN KEY CONSTRAINTS\n";
        for (const fk of use) {
          const cols  = fk.column_names.join(", ");
          const fcols = fk.foreign_column_names.join(", ");
          sql += `ALTER TABLE ${fk.table_schema}.${fk.table_name}\n`
               + `  ADD CONSTRAINT ${fk.fk_name}\n`
               + `  FOREIGN KEY (${cols}) REFERENCES ${fk.foreign_table_schema}.${fk.foreign_table_name}(${fcols});\n\n`;
        }
      }
    }

    // ─── Return SQL ─────────────────────────────────────────────────
    res.setHeader("Content-Type", "text/plain");
    res.status(200).send(sql);
  } catch (err: any) {
    console.error("generate-sql error:", err);
    res.status(500).send(err.message);
  }
}