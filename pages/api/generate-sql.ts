// pages/api/generate-sql.ts

import type { NextApiRequest, NextApiResponse } from "next";
import { createClient } from "@supabase/supabase-js";
import { TableSelection, FunctionSelection, TypeSelection, TriggerSelection } from "../../types";

export default async function handler(
  req: NextApiRequest,
  res: NextApiResponse<string | { error: string }>
) {
  try {
    // 1) creds
    const url = req.headers["x-sb-url"] as string;
    const key = req.headers["x-sb-key"] as string;
    if (!url || !key) throw new Error("Missing credentials");

    // RPC client for enums/tables/functions/triggers
    const rpc = createClient(url,key,{ global:{ headers:{ Accept:"application/json" }} });

    // 3) Parse selections
    const { selections, functionSelections = [], typeSelections = [], triggerSelections = [] } = req.body as { 
      selections: TableSelection[];
      functionSelections?: FunctionSelection[];
      typeSelections?: TypeSelection[];
      triggerSelections?: TriggerSelection[];
    };
    const picked = selections.filter((s) => s.selected);
    const pickedFunctions = functionSelections.filter((s) => s.selected);
    const pickedTypes = typeSelections.filter((s) => s.selected);
    const pickedTriggers = triggerSelections.filter((s) => s.selected);
    
    if (!picked.length && !pickedFunctions.length && !pickedTypes.length && !pickedTriggers.length) {
      throw new Error("No tables, functions, types, or triggers selected");
    }

    // Build sets for filtering
    const tableSet  = new Set(picked.map((s) => `${s.schema}.${s.table}`));
    const schemaSet = new Set(picked.map((s) => s.schema));

    // Helper to literalize JS values
    const lit = (v: any): string => {
      if (v == null) return "NULL";
      if (typeof v === "string") return `'${v.replace(/'/g,"''")}'`;
      if (typeof v === "object") {
        const js = JSON.stringify(v).replace(/'/g,"''");
        return `'${js}'::jsonb`;
      }
      return v.toString();
    };

    let sql = "-- Generated by Supabase Migration Tool\n\n";

    // ─── STEP 1: ENUM TYPES ─────────────────────────────────────────
    {
      const r = await rpc.rpc("pg_list_enum_types");
      if (r.error) throw r.error;
      const enums = r.data as Array<{
        type_schema: string;
        type_name:   string;
        labels:      string[];
      }> | null;

      const use = enums?.filter((e) => schemaSet.has(e.type_schema)) || [];
      if (use.length) {
        sql += "-- ENUM TYPES\n";
        for (const e of use) {
          const vals = e.labels.map((l) => `'${l.replace(/'/g,"''")}'`);
          sql += `CREATE TYPE ${e.type_schema}.${e.type_name} AS ENUM (${vals.join(
            ", "
          )});\n`;
        }
        sql += "\n";
      }
    }

    // ─── STEP 1.5: USER-DEFINED TYPES ──────────────────────────────────
    if (pickedTypes.length) {
      sql += "-- USER-DEFINED TYPES\n";
      for (const { schema, type } of pickedTypes) {
        try {
          const r = await rpc.rpc("pg_get_type_def", {
            schemaname: schema,
            typename: type
          });
          if (r.error) throw r.error;
          const typeDef = r.data as Array<{ definition: string }> | null;
          if (typeDef && typeDef.length > 0) {
            sql += `${typeDef[0].definition};\n\n`;
          } else {
            sql += `-- Could not retrieve definition for type ${schema}.${type}\n`;
          }
        } catch (err) {
          sql += `-- Error retrieving definition for type ${schema}.${type}: ${err}\n`;
        }
      }
      sql += "\n";
    }

    // ─── STEP 1.6: USER-DEFINED FUNCTIONS ──────────────────────────────
    if (pickedFunctions.length) {
      sql += "-- USER-DEFINED FUNCTIONS\n";
      for (const { schema, function: funcName } of pickedFunctions) {
        try {
          const r = await rpc.rpc("pg_get_function_def", {
            schemaname: schema,
            functionname: funcName
          });
          if (r.error) throw r.error;
          const funcDef = r.data as Array<{ definition: string }> | null;
          if (funcDef && funcDef.length > 0) {
            sql += `${funcDef[0].definition};\n\n`;
          } else {
            sql += `-- Could not retrieve definition for ${schema}.${funcName}\n`;
          }
        } catch (err) {
          sql += `-- Error retrieving definition for ${schema}.${funcName}: ${err}\n`;
        }
      }
      sql += "\n";
    }

    // ─── STEP 1.7: TRIGGERS ─────────────────────────────────────────────
    if (pickedTriggers.length) {
      sql += "-- TRIGGERS\n";
      for (const { schema, trigger, table } of pickedTriggers) {
        try {
          const r = await rpc.rpc("pg_get_trigger_def", {
            schemaname: schema,
            triggername: trigger
          });
          if (r.error) throw r.error;
          const triggerDef = r.data as Array<{ definition: string }> | null;
          if (triggerDef && triggerDef.length > 0) {
            sql += `${triggerDef[0].definition};\n\n`;
          } else {
            sql += `-- Could not retrieve definition for trigger ${schema}.${trigger} on ${table}\n`;
          }
        } catch (err) {
          sql += `-- Error retrieving definition for trigger ${schema}.${trigger}: ${err}\n`;
        }
      }
      sql += "\n";
    }

    // ─── STEP 2: DDL + DATA FOR EACH TABLE ───────────────────────────
    for (const { schema, table } of picked) {
      // 2a) DDL
      {
        const r = await rpc.rpc("pg_get_tabledef", {
          schemaname: schema,
          tablename:  table
        });
        if (error) throw error;
        sql += `${(data as any[])[0].def};\n\n`;
      }
    }

    // --- TABLE DDL + DATA ---
    for (const tbl of tables) {
      // DDL
      {
        const { data, error } = await rpc.rpc("pg_get_tabledef", {
          schemaname: tbl.schema,
          tablename:  tbl.name,
        });
        if (error) throw error;
        sql += `-- DDL for ${tbl.schema}.${tbl.name}\n${(data as any[])[0].ddl};\n\n`;
      }
      // Data
      {
        const db = createClient(url,key,{ db:{schema:tbl.schema}, global:{ headers:{Accept:"application/json"} }});
        let offset = 0, page=500;
        while (true) {
          const { data, error } = await db.from(tbl.name).select("*").range(offset,offset+page-1);
          if (error) throw error;
          const rows = data as any[];
          if (!rows.length) break;
          const cols = Object.keys(rows[0]);
          const vals = rows.map(r=>"("+
            cols.map(c=>
              r[c]===null?"NULL":
              typeof r[c]==="object"? `'${JSON.stringify(r[c]).replace(/'/g,"''")}'::jsonb`:
              `'${String(r[c]).replace(/'/g,"''")}'`
            ).join(", ")+")"
          );
          sql += `-- Data for ${tbl.schema}.${tbl.name} OFFSET ${offset}\n`;
          sql += `INSERT INTO ${tbl.schema}.${tbl.name} (${cols.join(", ")}) VALUES\n${vals.join(",\n")};\n\n`;
          if (rows.length<page) break;
          offset+=page;
        }
      }
    }

    // --- CONSTRAINTS ---
    for (const tbl of tables) {
      const { data, error } = await rpc.rpc("pg_list_constraints", {
        schemaname: tbl.schema,
        tablename:  tbl.name,
      });
      if (error) throw error;
      for (const c of data as any[]) {
        sql += `ALTER TABLE ${tbl.schema}.${tbl.name}\n  ADD CONSTRAINT ${c.constraint_name} ${c.definition};\n`;
      }
      sql += "\n";
    }

    // --- INDEXES (skip pkey/indexes matching constraint names) ---
    for (const tbl of tables) {
      // get constraint names
      const rc = await rpc.rpc("pg_list_constraints", {
        schemaname: tbl.schema,
        tablename:  tbl.name,
      });
      if (rc.error) throw rc.error;
      const cNames = (rc.data as any[]).map((r)=>r.constraint_name);

      // get indexes
      const ri = await rpc.rpc("pg_list_indexes", {
        schemaname: tbl.schema,
        tablename:  tbl.name,
      });
      if (ri.error) throw ri.error;
      const idxs = ri.data as Array<{ indexdef:string }>;

      const use = idxs.filter(ix=>{
        const m = ix.indexdef.match(/INDEX\s+("?)([^\s"]+)\1/i);
        const nm = m?.[2]||"";
        return nm && !cNames.includes(nm);
      });

      if (use.length) {
        sql+=`-- Indexes for ${tbl.schema}.${tbl.name}\n`;
        use.forEach(ix=>sql+=ix.indexdef.trim()+";\n");
        sql+="\n";
      }
    }

    // --- FOREIGN KEYS between selected tables ---
    {
      const r = await rpc.rpc("pg_list_foreign_keys");
      if (r.error) throw r.error;
      const fks = r.data as any[];

      const use = fks.filter((fk)=>{
        const left  = `${fk.table_schema}.${fk.table_name}`;
        const right = `${fk.foreign_table_schema}.${fk.foreign_table_name}`;
        return tableSet.has(left) && tableSet.has(right);
      });

      if (use.length) {
        sql+="-- FOREIGN KEY CONSTRAINTS\n";
        use.forEach((fk:any)=>{
          const cols  = fk.column_names.join(", ");
          const fcols = fk.foreign_column_names.join(", ");
          sql+=`ALTER TABLE ${fk.table_schema}.${fk.table_name}\n`+
               `  ADD CONSTRAINT ${fk.fk_name}\n`+
               `  FOREIGN KEY (${cols}) REFERENCES ${fk.foreign_table_schema}.${fk.foreign_table_name}(${fcols});\n\n`;
        });
      }
    }

    res.setHeader("Content-Type","text/plain");
    return res.status(200).send(sql);
  }
  catch(err:any){
    console.error("generate-sql error:",err);
    return res.status(500).send(err.message);
  }
}