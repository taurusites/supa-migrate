// services/supabaseService.ts
import { createClient, SupabaseClient } from "@supabase/supabase-js";
import { SchemaInfo, TableSelection } from "../types";

// RPC return shapes
interface EnumTypeRow {
  type_schema: string;
  type_name:   string;
  labels:      string[];
}
interface FKRow {
  fk_schema:            string;
  fk_name:              string;
  table_schema:         string;
  table_name:           string;
  column_names:         string[];
  foreign_table_schema: string;
  foreign_table_name:   string;
  foreign_column_names: string[];
}
interface ConstraintRow {
  constraint_name: string;
  definition:      string;
}
interface IndexRow {
  indexdef: string;
}

/**
 * Create a Supabase client.
 * If `schema` is provided, all .from() calls will target that schema.
 */
function getClient(
  url: string,
  key: string,
  schema: string = "public"
): SupabaseClient {
  return createClient(url, key, {
    // PostgREST schema override:
    db: { schema },
    // Accept JSON everywhere
    global: { headers: { Accept: "application/json" } },
  });
}

/** 1) List all non-system schemas + their tables. */
export async function listSchemasAndTables(
  url: string,
  key: string
): Promise<SchemaInfo[]> {
  // Use default‐schema client for RPCs
  const supa = getClient(url, key);

  // a) fetch schemas
  const { data: scRows, error: scErr } = await supa.rpc("pg_list_schemas");
  if (scErr) throw scErr;
  const schemas = (scRows as any[] | null)
    ?.map((r) => (r as any).schema_name as string)
    .filter(Boolean) || [];

  // b) fetch tables per schema
  const out: SchemaInfo[] = [];
  for (const schema of schemas) {
    const { data: tbRows, error: tbErr } = await supa.rpc(
      "pg_list_tables",
      { schemaname: schema }
    );
    if (tbErr) throw tbErr;
    const tables = (tbRows as any[] | null)
      ?.map((r) => (r as any).table_name as string)
      .filter(Boolean) || [];
    out.push({ schema, tables });
  }

  return out;
}

/** 2) Generate a complete migration SQL script. */
export async function generateMigrationSQL(
  url: string,
  key: string,
  selections: TableSelection[]
): Promise<string> {
  // RPC‐client (always default public schema)
  const rpcClient = getClient(url, key, "public");
  let sql = "-- Generated by Supabase Migration Tool\n\n";

  // --- ENUM TYPES ---
  {
    const { data: enums, error: eErr } = await rpcClient.rpc(
      "pg_list_enum_types"
    );
    if (eErr) throw eErr;
    (enums as EnumTypeRow[] | null)?.forEach((e) => {
      const vals = e.labels.map((l) => `'${l.replace(/'/g, "''")}'`);
      sql += `CREATE TYPE ${e.type_schema}.${e.type_name} AS ENUM (${vals.join(", ")});\n`;
    });
    if (enums && enums.length) sql += "\n";
  }

  // --- TABLE DDL + CONSTRAINTS + INDEXES ---
  for (const sel of selections.filter((s) => s.selected)) {
    const { schema, table } = sel;

    // a) CREATE TABLE DDL
    {
      const { data: ddlRows, error: dErr } = await rpcClient.rpc(
        "pg_get_tabledef",
        { schemaname: schema, tablename: table }
      );
      if (dErr) throw dErr;
      const ddl = (ddlRows as any[])[0]?.ddl as string;
      sql += `-- DDL for ${schema}.${table}\n${ddl};\n\n`;
    }

    // b) PRIMARY & UNIQUE constraints
    {
      const { data: cons, error: cErr } = await rpcClient.rpc(
        "pg_list_constraints",
        { schemaname: schema, tablename: table }
      );
      if (cErr) throw cErr;
      (cons as ConstraintRow[] | null)?.forEach((c) => {
        sql += `ALTER TABLE ${schema}.${table}\n  ADD CONSTRAINT ${c.constraint_name} ${c.definition};\n`;
      });
      if (cons && cons.length) sql += "\n";
    }

    // c) Non-constraint INDEXes
    {
      const { data: idxs, error: iErr } = await rpcClient.rpc(
        "pg_list_indexes",
        { schemaname: schema, tablename: table }
      );
      if (iErr) throw iErr;
      (idxs as IndexRow[] | null)?.forEach((ix) => {
        sql += ix.indexdef.trim() + ";\n";
      });
      if (idxs && idxs.length) sql += "\n";
    }
  }

  // --- FOREIGN KEYS ---
  {
    const { data: fks, error: fkErr } = await rpcClient.rpc(
      "pg_list_foreign_keys"
    );
    if (fkErr) throw fkErr;
    (fks as FKRow[] | null)?.forEach((fk) => {
      const cols  = fk.column_names.join(", ");
      const fcols = fk.foreign_column_names.join(", ");
      sql += `ALTER TABLE ${fk.table_schema}.${fk.table_name}\n`
           + `  ADD CONSTRAINT ${fk.fk_name}\n`
           + `  FOREIGN KEY (${cols}) REFERENCES ${fk.foreign_table_schema}.${fk.foreign_table_name}(${fcols});\n\n`;
    });
  }

  // --- TABLE DATA INSERTS (paginated) ---
  for (const sel of selections.filter((s) => s.selected)) {
    const { schema, table } = sel;
    const pageSize = 500;
    let offset = 0;

    // Create a client scoped to this schema so `.from(table)` hits exactly schema.table
    const dataClient = getClient(url, key, schema);

    while (true) {
      const { data: batch, error: dErr } = await dataClient
        .from(table)
        .select("*")
        .range(offset, offset + pageSize - 1);

      if (dErr) throw dErr;
      if (!batch || batch.length === 0) break;

      const cols = Object.keys(batch[0]);
      const vals = (batch as any[]).map((row) => {
        const parts = cols.map((c) => {
          const v = row[c];
          if (v === null) return "NULL";
          if (typeof v === "string") return `'${v.replace(/'/g, "''")}'`;
          return v.toString();
        });
        return `(${parts.join(", ")})`;
      });

      sql += `-- Data for ${schema}.${table} OFFSET ${offset}\n`;
      sql += `INSERT INTO ${schema}.${table} (${cols.join(
        ", "
      )}) VALUES\n${vals.join(",\n")};\n\n`;

      if ((batch as any[]).length < pageSize) break;
      offset += pageSize;
    }
  }

  return sql;
}