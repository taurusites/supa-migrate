// services/supabaseService.ts
import { createClient, SupabaseClient } from "@supabase/supabase-js";
import { SchemaInfo, TableSelection } from "../types";

// --- Row shapes for casting RPC results ---
interface EnumTypeRow {
  type_schema: string;
  type_name:   string;
  labels:      string[];
}

interface FKRow {
  fk_schema:            string;
  fk_name:              string;
  table_schema:         string;
  table_name:           string;
  column_names:         string[];
  foreign_table_schema: string;
  foreign_table_name:   string;
  foreign_column_names: string[];
}

interface ConstraintRow {
  constraint_name: string;
  definition:      string;
}

interface IndexRow {
  indexdef: string;
}

// --- Supabase client factory ---
function getClient(url: string, key: string): SupabaseClient {
  return createClient(url, key, {
    global: { headers: { Accept: "application/json" } },
  });
}

// --------------------------------------------------
// 1) List all schemas & their tables via RPC calls
// --------------------------------------------------
export async function listSchemasAndTables(
  url: string,
  key: string
): Promise<SchemaInfo[]> {
  const supa = getClient(url, key);

  // a) fetch schema names
  const { data: scRows, error: scErr } = await supa.rpc("pg_list_schemas");
  if (scErr) throw scErr;
  const schemas = (scRows as any[] | null)
    ?.map((r) => (r as any).schema_name as string)
    .filter(Boolean) || [];

  // b) for each schema, fetch its tables
  const result: SchemaInfo[] = [];
  for (const schema of schemas) {
    const { data: tbRows, error: tbErr } = await supa.rpc(
      "pg_list_tables",
      { schemaname: schema }
    );
    if (tbErr) throw tbErr;
    const tables = (tbRows as any[] | null)
      ?.map((r) => (r as any).table_name as string)
      .filter(Boolean) || [];
    result.push({ schema, tables });
  }

  return result;
}

// ------------------------------------------------------------------
// 2) Generate full migration SQL: enums, DDL, constraints, indexes,
//    FKs, then data inserts (paginated)
// ------------------------------------------------------------------
export async function generateMigrationSQL(
  url: string,
  key: string,
  selections: TableSelection[]
): Promise<string> {
  const supa = getClient(url, key);
  let sql = "-- Generated by Supabase Migration Tool\n\n";

  // 2.1) ENUM types
  {
    const { data: enumRows, error: enumErr } = await supa.rpc(
      "pg_list_enum_types"
    );
    if (enumErr) throw enumErr;
    const enums = enumRows as EnumTypeRow[] | null;
    if (enums && enums.length) {
      sql += "-- ENUM TYPES\n";
      for (const e of enums) {
        const vals = e.labels.map((l) => `'${l.replace(/'/g, "''")}'`);
        sql += `CREATE TYPE ${e.type_schema}.${e.type_name} AS ENUM (${vals.join(
          ", "
        )});\n`;
      }
      sql += "\n";
    }
  }

  // 2.2) Table DDL, constraints & indexes
  for (const sel of selections.filter((s) => s.selected)) {
    const { schema, table } = sel;

    // a) CREATE TABLE DDL
    {
      const { data: ddlRows, error: ddlErr } = await supa.rpc(
        "pg_get_tabledef",
        { schemaname: schema, tablename: table }
      );
      if (ddlErr) throw ddlErr;
      const ddl = (ddlRows as any[])[0]?.ddl as string;
      sql += `-- DDL for ${schema}.${table}\n${ddl};\n\n`;
    }

    // b) PRIMARY & UNIQUE constraints
    {
      const { data: cRows, error: cErr } = await supa.rpc(
        "pg_list_constraints",
        { schemaname: schema, tablename: table }
      );
      if (cErr) throw cErr;
      const cons = cRows as ConstraintRow[] | null;
      if (cons && cons.length) {
        sql += `-- Constraints for ${schema}.${table}\n`;
        for (const c of cons) {
          sql += `ALTER TABLE ${schema}.${table}\n`
               + `  ADD CONSTRAINT ${c.constraint_name} ${c.definition};\n`;
        }
        sql += "\n";
      }
    }

    // c) Non‐constraint indexes
    {
      const { data: iRows, error: iErr } = await supa.rpc(
        "pg_list_indexes",
        { schemaname: schema, tablename: table }
      );
      if (iErr) throw iErr;
      const idxs = iRows as IndexRow[] | null;
      if (idxs && idxs.length) {
        sql += `-- Indexes for ${schema}.${table}\n`;
        for (const ix of idxs) {
          sql += ix.indexdef.trim() + ";\n";
        }
        sql += "\n";
      }
    }
  }

  // 2.3) All FOREIGN KEY constraints
  {
    const { data: fkRows, error: fkErr } = await supa.rpc(
      "pg_list_foreign_keys"
    );
    if (fkErr) throw fkErr;
    const fks = fkRows as FKRow[] | null;
    if (fks && fks.length) {
      sql += "-- FOREIGN KEY CONSTRAINTS\n";
      for (const fk of fks) {
        const cols = fk.column_names.join(", ");
        const fcols = fk.foreign_column_names.join(", ");
        sql += `ALTER TABLE ${fk.table_schema}.${fk.table_name}\n`
             + `  ADD CONSTRAINT ${fk.fk_name}\n`
             + `  FOREIGN KEY (${cols}) REFERENCES ${fk.foreign_table_schema}.${fk.foreign_table_name}(${fcols});\n\n`;
      }
    }
  }

  // 2.4) Data INSERTS with pagination
  for (const sel of selections.filter((s) => s.selected)) {
    const { schema, table } = sel;
    const pageSize = 500;
    let offset = 0;

    while (true) {
      // <-- fixed: pass "schema.table" as single arg to .from()
      const { data: batch, error: dataErr } = await supa
        .from(`${schema}.${table}`)
        .select("*")
        .range(offset, offset + pageSize - 1);

      if (dataErr) throw dataErr;
      if (!batch || batch.length === 0) break;

      const cols = Object.keys(batch[0]);
      const values = batch.map((row) => {
        const parts = cols.map((c) => {
          const v = (row as any)[c];
          if (v === null) return "NULL";
          if (typeof v === "string") return `'${v.replace(/'/g, "''")}'`;
          return v.toString();
        });
        return `(${parts.join(", ")})`;
      });

      sql += `-- Data for ${schema}.${table} OFFSET ${offset}\n`;
      sql += `INSERT INTO ${schema}.${table} (${cols.join(
        ", "
      )}) VALUES\n${values.join(",\n")};\n\n`;

      if (batch.length < pageSize) break;
      offset += pageSize;
    }
  }

  return sql;
}